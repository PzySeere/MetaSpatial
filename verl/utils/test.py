import os
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoProcessor

from dataset import RLHFDataset, collate_fn  # æ ¹æ®ä½ çš„è·¯å¾„ä¿®æ”¹è¿™ä¸ª import


def build_single_sample_batch(data: dict, tokenizer, processor) -> dict:
    dataset = RLHFDataset(
        data_path="",  # å ä½ï¼Œä¸ä¼šå®é™…ç”¨åˆ°
        tokenizer=tokenizer,
        processor=processor,
        prompt_key="prompt",
        answer_key="answer",
        image_key="images",
        max_prompt_length=512,
        truncation="right",
        system_prompt=None,
        max_pixels=1024 * 1024,
        min_pixels=64 * 64,
    )
    # è¦†ç›–å†…éƒ¨ dataset
    dataset.dataset = Dataset.from_dict(data)
    return collate_fn([dataset[0]])


def test_single_sample_batch():
    # æ›¿æ¢ä¸ºä½ æœ¬åœ°çš„ä¸€å¼ å›¾ç‰‡è·¯å¾„
    image_path = "/projects/p32364/MetaSpatial/curated_data/room_2121/render_output.png"
    assert os.path.exists(image_path), f"Image not found: {image_path}"

    with open(image_path, "rb") as f:
        image_bytes = f.read()

    # æ„é€ ä¸€æ¡æµ‹è¯•æ•°æ®
    data = {
        "prompt": ["Place a bed near the window."],
        "answer": ["The bed is now placed near the window."],
        "images": [[{"bytes": image_bytes, "path": os.path.basename(image_path)}]],
    }

    # åˆå§‹åŒ– tokenizer å’Œ processor
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat")
    processor = AutoProcessor.from_pretrained("Qwen/Qwen-VL-Chat")

    # æ„é€  batch
    batch = build_single_sample_batch(data, tokenizer, processor)

    # åŸºæœ¬æ£€æŸ¥
    print("âœ… Keys in batch:", batch.keys())
    assert "input_ids" in batch
    assert "attention_mask" in batch
    assert "position_ids" in batch
    assert "ground_truth" in batch
    assert batch["input_ids"].shape[0] == 1
    assert batch["attention_mask"].shape[0] == 1
    assert isinstance(batch["input_ids"], torch.Tensor)
    assert isinstance(batch["ground_truth"][0], str)

    print("ğŸ‰ Single sample batch is valid.")
    print("Decoded prompt:\n", tokenizer.decode(batch["input_ids"][0], skip_special_tokens=True))
    print("Ground truth:\n", batch["ground_truth"][0])


if __name__ == "__main__":
    test_single_sample_batch()
